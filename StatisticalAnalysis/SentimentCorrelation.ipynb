{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Make sure you have generated rollingMean parquet files according to SentimentPreprocessing notebook and upload to use this notebook. NB: File paths specified in this notebook assume you are using a google colab environment, rename paths accordingly if you would like to save output data somewhere specific."
      ],
      "metadata": {
        "id": "NfDzHqICqzN_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhNALMRA9hLQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# List of popular symbols\n",
        "popular_symbols = [\n",
        "    \"AAPL\", \"MSFT\", \"TSLA\", \"AMZN\", \"NVDA\", \"NFLX\",\n",
        "    \"META\", \"IBM\", \"MCD\", \"NKE\", \"SBUX\", \"MS\", \"JPM\",\n",
        "    \"EBAY\", \"COST\", \"GE\", \"BA\", \"PYPL\", \"GS\",\n",
        "]\n",
        "\n",
        "# Load transitions data from the three Parquet files\n",
        "news_df = pd.read_parquet('rollingNews.parquet')\n",
        "submissions_df = pd.read_parquet('rollingSubmissions.parquet')\n",
        "comments_df = pd.read_parquet('rollingComments.parquet')\n",
        "\n",
        "# Ensure the date column is in datetime format\n",
        "for df in [news_df, submissions_df, comments_df]:\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Filter each DataFrame for only the popular symbols\n",
        "news_df = news_df[news_df['symbol'].isin(popular_symbols)]\n",
        "submissions_df = submissions_df[submissions_df['symbol'].isin(popular_symbols)]\n",
        "comments_df = comments_df[comments_df['symbol'].isin(popular_symbols)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation matrix between periods per sentiment type per company"
      ],
      "metadata": {
        "id": "kHzY3r0-Cnou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr"
      ],
      "metadata": {
        "id": "hseuEBZl-iKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store results per company\n",
        "resultsNews = {}\n",
        "\n",
        "# Process each company (symbol) separately\n",
        "for symbol, group in news_df.groupby('symbol'):\n",
        "    # Sort by date to ensure correct order\n",
        "    group_sorted = group.sort_values('date').reset_index(drop=True)\n",
        "    # Check if there are at least two periods to compare\n",
        "    if len(group_sorted) < 2:\n",
        "        print(f\"Not enough periods for symbol: {symbol}\")\n",
        "        continue\n",
        "\n",
        "    # Extract the sentiment series\n",
        "    sentiment = group_sorted['news_sentiment']\n",
        "    # Create two series:\n",
        "    #   - current: sentiment for periods 1 to N-1\n",
        "    #   - next_period: sentiment for periods 2 to N\n",
        "    current = sentiment.iloc[:-1].reset_index(drop=True)\n",
        "    next_period = sentiment.iloc[1:].reset_index(drop=True)\n",
        "    # Compute the Pearson correlation between consecutive periods\n",
        "    corr, p_value = pearsonr(current, next_period)\n",
        "    resultsNews[symbol] = {'correlation': corr, 'p_value': p_value}\n",
        "    print(f\"Symbol: {symbol} -> Pearson correlation between consecutive periods: {corr:.4f}, p-value: {p_value:.4g}\")\n",
        "\n",
        "# If needed, you can inspect the results dictionary for all symbols\n",
        "print(\"\\nAll results:\")\n",
        "print(resultsNews)"
      ],
      "metadata": {
        "id": "pmBUqVWBAY0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store results per company\n",
        "resultsSubmissions = {}\n",
        "\n",
        "# Process each company (symbol) separately\n",
        "for symbol, group in submissions_df.groupby('symbol'):\n",
        "    # Sort by date to ensure correct order\n",
        "    group_sorted = group.sort_values('date').reset_index(drop=True)\n",
        "    # Check if there are at least two periods to compare\n",
        "    if len(group_sorted) < 2:\n",
        "        print(f\"Not enough periods for symbol: {symbol}\")\n",
        "        continue\n",
        "\n",
        "    # Extract the sentiment series\n",
        "    sentiment = group_sorted['submissions_sentiment']\n",
        "    # Create two series:\n",
        "    #   - current: sentiment for periods 1 to N-1\n",
        "    #   - next_period: sentiment for periods 2 to N\n",
        "    current = sentiment.iloc[:-1].reset_index(drop=True)\n",
        "    next_period = sentiment.iloc[1:].reset_index(drop=True)\n",
        "    # Compute the Pearson correlation between consecutive periods\n",
        "    corr, p_value = pearsonr(current, next_period)\n",
        "    resultsSubmissions[symbol] = {'correlation': corr, 'p_value': p_value}\n",
        "    print(f\"Symbol: {symbol} -> Pearson correlation between consecutive periods: {corr:.4f}, p-value: {p_value:.4g}\")\n",
        "\n",
        "# If needed, you can inspect the results dictionary for all symbols\n",
        "print(\"\\nAll results:\")\n",
        "print(resultsSubmissions)"
      ],
      "metadata": {
        "id": "WPJb-Tq8Ae_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store results per company\n",
        "resultsComments = {}\n",
        "\n",
        "# Process each company (symbol) separately\n",
        "for symbol, group in comments_df.groupby('symbol'):\n",
        "    # Sort by date to ensure correct order\n",
        "    group_sorted = group.sort_values('date').reset_index(drop=True)\n",
        "    # Check if there are at least two periods to compare\n",
        "    if len(group_sorted) < 2:\n",
        "        print(f\"Not enough periods for symbol: {symbol}\")\n",
        "        continue\n",
        "\n",
        "    # Extract the sentiment series\n",
        "    sentiment = group_sorted['comments_sentiment']\n",
        "    # Create two series:\n",
        "    #   - current: sentiment for periods 1 to N-1\n",
        "    #   - next_period: sentiment for periods 2 to N\n",
        "    current = sentiment.iloc[:-1].reset_index(drop=True)\n",
        "    next_period = sentiment.iloc[1:].reset_index(drop=True)\n",
        "    # Compute the Pearson correlation between consecutive periods\n",
        "    corr, p_value = pearsonr(current, next_period)\n",
        "    resultsComments[symbol] = {'correlation': corr, 'p_value': p_value}\n",
        "    print(f\"Symbol: {symbol} -> Pearson correlation between consecutive periods: {corr:.4f}, p-value: {p_value:.4g}\")\n",
        "\n",
        "# If needed, you can inspect the results dictionary for all symbols\n",
        "print(\"\\nAll results:\")\n",
        "print(resultsComments)"
      ],
      "metadata": {
        "id": "04YOefpS-LY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract symbols and correlation coefficients\n",
        "symbols = list(resultsNews.keys())\n",
        "correlations = [resultsNews[sym]['correlation'] for sym in symbols]\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "bars = plt.bar(symbols, correlations, color='skyblue')\n",
        "\n",
        "# Add correlation values on top of each bar\n",
        "for bar, corr in zip(bars, correlations):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{corr:.2f}',\n",
        "             ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.xlabel('Company Symbol')\n",
        "plt.ylabel('Pearson Correlation')\n",
        "plt.title('Correlation Between Consecutive 30-Day News Sentiment Periods per Company')\n",
        "plt.ylim(0, 1.05)  # Set y-limit to provide some space above bars\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-63-SlZbAQb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract symbols and correlation coefficients\n",
        "symbols = list(resultsSubmissions.keys())\n",
        "correlations = [resultsSubmissions[sym]['correlation'] for sym in symbols]\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "bars = plt.bar(symbols, correlations, color='skyblue')\n",
        "\n",
        "# Add correlation values on top of each bar\n",
        "for bar, corr in zip(bars, correlations):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{corr:.2f}',\n",
        "             ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.xlabel('Company Symbol')\n",
        "plt.ylabel('Pearson Correlation')\n",
        "plt.title('Correlation Between Consecutive 30-Day Reddit Submissions Sentiment Periods per Company')\n",
        "plt.ylim(0, 1.05)  # Set y-limit to provide some space above bars\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JGDiTrM4AwnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract symbols and correlation coefficients\n",
        "symbols = list(resultsComments.keys())\n",
        "correlations = [resultsComments[sym]['correlation'] for sym in symbols]\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "bars = plt.bar(symbols, correlations, color='skyblue')\n",
        "\n",
        "# Add correlation values on top of each bar\n",
        "for bar, corr in zip(bars, correlations):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{corr:.2f}',\n",
        "             ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.xlabel('Company Symbol')\n",
        "plt.ylabel('Pearson Correlation')\n",
        "plt.title('Correlation Between Consecutive 30-Day Reddit Comments Sentiment Periods per Company')\n",
        "plt.ylim(0, 1.05)  # Set y-limit to provide some space above bars\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wg_-w5FlBAfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge the data frames"
      ],
      "metadata": {
        "id": "VU0DaWJ9CSXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To merge, we assume the columns holding transition values are named:\n",
        "# 'news_transition' in news_df, 'submissions_transition' in submissions_df, and 'comments_transition' in comments_df.\n",
        "# If they are named differently, rename them accordingly.\n",
        "\n",
        "# Merge the DataFrames on 'symbol' and 'date'\n",
        "merged_df = pd.merge(news_df, submissions_df, on=['symbol', 'date'], how='outer')\n",
        "merged_df = pd.merge(merged_df, comments_df, on=['symbol', 'date'], how='outer')\n",
        "\n",
        "# Sort and fill missing values (if desired, e.g., with 0)\n",
        "merged_df.sort_values(['symbol', 'date'], inplace=True)\n",
        "merged_df.fillna(0, inplace=True)\n",
        "\n",
        "# Display a preview of the merged data\n",
        "print(\"Merged Data Preview:\")\n",
        "display(merged_df.head())"
      ],
      "metadata": {
        "id": "OKSQBVA8-Iug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation Matrix between each sentiment type per company"
      ],
      "metadata": {
        "id": "5T2rnozOChvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for symbol in popular_symbols:\n",
        "    symbol_data = merged_df[merged_df['symbol'] == symbol]\n",
        "    if symbol_data.empty:\n",
        "        continue\n",
        "\n",
        "    corr_matrix = symbol_data[['news_sentiment', 'submissions_sentiment', 'comments_sentiment']].corr()\n",
        "    print(f\"Correlation matrix for {symbol}:\")\n",
        "    print(corr_matrix)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "gK4EpYbMCQ0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pairwise Correlation Plots"
      ],
      "metadata": {
        "id": "cD-2WHctEEf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the output directory for pairplots in /content/\n",
        "output_dir = \"/content/pairplots\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Loop through each popular symbol and generate pairwise scatter plots\n",
        "for symbol in popular_symbols:\n",
        "    # Filter merged_df for the current symbol\n",
        "    symbol_data = merged_df[merged_df['symbol'] == symbol]\n",
        "    if symbol_data.empty:\n",
        "        continue  # Skip symbols with no data\n",
        "\n",
        "    # Create a pairplot using only the transition columns\n",
        "    pair_grid = sns.pairplot(symbol_data[['news_sentiment', 'submissions_sentiment', 'comments_sentiment']])\n",
        "    pair_grid.fig.suptitle(f\"Pairwise Scatter Plots for {symbol}\", y=1.02)\n",
        "\n",
        "    # Save the pairplot to the output directory\n",
        "    file_path = os.path.join(output_dir, f\"pairplot_{symbol}.png\")\n",
        "    pair_grid.savefig(file_path)\n",
        "\n",
        "    # Close the plot to free memory\n",
        "    plt.close('all')"
      ],
      "metadata": {
        "id": "C7h_Lh8DC36k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the folder containing the pairplots\n",
        "!zip -r /content/pairplots.zip /content/pairplots"
      ],
      "metadata": {
        "id": "a6vB0Ad4C-kL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary Statistics"
      ],
      "metadata": {
        "id": "WGJPQlePEBdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for symbol in popular_symbols:\n",
        "    symbol_data = merged_df[merged_df['symbol'] == symbol]\n",
        "    if symbol_data.empty:\n",
        "        continue\n",
        "\n",
        "    stats = symbol_data[['news_sentiment', 'submissions_sentiment', 'comments_sentiment']].describe()\n",
        "    print(f\"Descriptive statistics for {symbol}:\")\n",
        "    print(stats)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "hWN_ykFbD1eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rolling window analysis\n",
        "\n",
        "Identify periods when rolling means diverge between news and Reddit channels. Sudden shifts may indicate market events or changes in sentiment drivers."
      ],
      "metadata": {
        "id": "ULX1BjE6Eflv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rolling_dir = '/content/rolling_plots'\n",
        "os.makedirs(rolling_dir, exist_ok=True)\n",
        "\n",
        "rolling_window = 5  # Adjust the window size as desired\n",
        "\n",
        "for symbol in popular_symbols:\n",
        "    symbol_data = merged_df[merged_df['symbol'] == symbol].copy().sort_values('date')\n",
        "    if symbol_data.empty:\n",
        "        continue\n",
        "\n",
        "    # Compute rolling means for each sentiment transition measure\n",
        "    symbol_data['news_roll_mean'] = symbol_data['news_sentiment'].rolling(window=rolling_window).mean()\n",
        "    symbol_data['submissions_roll_mean'] = symbol_data['submissions_sentiment'].rolling(window=rolling_window).mean()\n",
        "    symbol_data['comments_roll_mean'] = symbol_data['comments_sentiment'].rolling(window=rolling_window).mean()\n",
        "\n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(symbol_data['date'], symbol_data['news_roll_mean'], label='News Rolling Mean')\n",
        "    plt.plot(symbol_data['date'], symbol_data['submissions_roll_mean'], label='Submissions Rolling Mean')\n",
        "    plt.plot(symbol_data['date'], symbol_data['comments_roll_mean'], label='Comments Rolling Mean')\n",
        "    plt.title(f\"Rolling Means (window={rolling_window}) for {symbol}\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Transition Value (Rolling Mean)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Save the plot as PNG\n",
        "    plot_filename = os.path.join(rolling_dir, f\"rolling_{symbol}.png\")\n",
        "    plt.savefig(plot_filename)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "izfrt6lOEVOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the folder containing the pairplots\n",
        "!zip -r /content/rolling_plots.zip /content/rolling_plots"
      ],
      "metadata": {
        "id": "HklvaZ_6Ed63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression:\n",
        "\n",
        "Look at the regression coefficients and their significance. Significant positive (or negative) coefficients for Reddit submissions or comments transitions suggest that changes in Reddit sentiment are statistically associated with changes in news sentiment. Compare R-squared values across symbols to gauge how well the model explains the variation in news sentiment."
      ],
      "metadata": {
        "id": "8g4g5whaFeM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes in Reddit effect News\n",
        "\n",
        "\n",
        "\n",
        "*   Dependent: News\n",
        "*   Predictor: Reddit Comments  & Reddit submissions\n",
        "\n"
      ],
      "metadata": {
        "id": "5F-oyue4F1Ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.tsa.stattools as tsastat\n",
        "import seaborn as sns\n",
        "import io\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "regression_dir = '/content/regression_RedditOnNews'\n",
        "os.makedirs(regression_dir, exist_ok=True)\n",
        "\n",
        "for symbol in popular_symbols:\n",
        "    symbol_data = merged_df[merged_df['symbol'] == symbol].copy().sort_values('date')\n",
        "    # Drop rows with missing transitions\n",
        "    symbol_data = symbol_data.dropna(subset=['news_sentiment', 'submissions_sentiment', 'comments_sentiment'])\n",
        "    if symbol_data.empty:\n",
        "        continue\n",
        "\n",
        "    # Set up regression: news_transition ~ submissions_transition + comments_transition\n",
        "    y = symbol_data['news_sentiment']\n",
        "    X = symbol_data[['submissions_sentiment', 'comments_sentiment']]\n",
        "    X = sm.add_constant(X)  # add constant\n",
        "    model = sm.OLS(y, X).fit()\n",
        "\n",
        "    # Get the model summary as text\n",
        "    summary_str = model.summary().as_text()\n",
        "\n",
        "    # Save to a text file\n",
        "    regression_filename = os.path.join(regression_dir, f\"regression_{symbol}.txt\")\n",
        "    with open(regression_filename, 'w') as f:\n",
        "        f.write(summary_str)"
      ],
      "metadata": {
        "id": "xU8QN9BbF0CJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the folder containing the pairplots\n",
        "!zip -r /content/regression_RedditOnNews.zip /content/regression_RedditOnNews"
      ],
      "metadata": {
        "id": "_vCrOL_xGhWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes in News effect Reddit\n",
        "\n",
        "*   Dependent: Reddit Submission\n",
        "*   Predictor: News\n",
        "\n"
      ],
      "metadata": {
        "id": "HfAKBSgSGawa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import statsmodels.api as sm\n",
        "\n",
        "regression_dir = '/content/regression_NewsOnRedditSubmissions'\n",
        "os.makedirs(regression_dir, exist_ok=True)\n",
        "\n",
        "for symbol in popular_symbols:\n",
        "    # Filter and sort the data by date for the current symbol\n",
        "    symbol_data = merged_df[merged_df['symbol'] == symbol].copy().sort_values('date')\n",
        "    # Drop rows with missing values for news_sentiment or submissions_sentiment\n",
        "    symbol_data = symbol_data.dropna(subset=['news_sentiment', 'submissions_sentiment'])\n",
        "    if symbol_data.empty:\n",
        "        continue\n",
        "\n",
        "    # Set up regression: reddit_submissions ~ news_sentiment\n",
        "    # Independent variable: news sentiment, with a constant added\n",
        "    X = sm.add_constant(symbol_data['news_sentiment'])\n",
        "    # Dependent variable: reddit submissions sentiment\n",
        "    y = symbol_data['submissions_sentiment']\n",
        "\n",
        "    # Fit the OLS regression model\n",
        "    model = sm.OLS(y, X).fit()\n",
        "\n",
        "    # Get the model summary as text\n",
        "    summary_str = model.summary().as_text()\n",
        "\n",
        "    # Save the summary to a text file\n",
        "    regression_filename = os.path.join(regression_dir, f\"regression_{symbol}.txt\")\n",
        "    with open(regression_filename, 'w') as f:\n",
        "        f.write(summary_str)"
      ],
      "metadata": {
        "id": "HDRzrDnCGogn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the folder containing the pairplots\n",
        "!zip -r /content/regression_NewsOnRedditSubmissions.zip /content/regression_NewsOnRedditSubmissions"
      ],
      "metadata": {
        "id": "AZQFcU6kJ45c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes in News effect Reddit\n",
        "\n",
        "*   Dependent: Reddit Comments\n",
        "*   Predictor: News"
      ],
      "metadata": {
        "id": "6xYcWdO5KELd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import statsmodels.api as sm\n",
        "\n",
        "regression_dir = '/content/regression_NewsOnComments'\n",
        "os.makedirs(regression_dir, exist_ok=True)\n",
        "\n",
        "for symbol in popular_symbols:\n",
        "    # Filter and sort the data by date for the current symbol\n",
        "    symbol_data = merged_df[merged_df['symbol'] == symbol].copy().sort_values('date')\n",
        "    # Drop rows with missing values for news_sentiment or submissions_sentiment\n",
        "    symbol_data = symbol_data.dropna(subset=['news_sentiment', 'comments_sentiment'])\n",
        "    if symbol_data.empty:\n",
        "        continue\n",
        "\n",
        "    # Set up regression: reddit_submissions ~ news_sentiment\n",
        "    # Independent variable: news sentiment, with a constant added\n",
        "    X = sm.add_constant(symbol_data['news_sentiment'])\n",
        "    # Dependent variable: reddit submissions sentiment\n",
        "    y = symbol_data['comments_sentiment']\n",
        "\n",
        "    # Fit the OLS regression model\n",
        "    model = sm.OLS(y, X).fit()\n",
        "\n",
        "    # Get the model summary as text\n",
        "    summary_str = model.summary().as_text()\n",
        "\n",
        "    # Save the summary to a text file\n",
        "    regression_filename = os.path.join(regression_dir, f\"regression_{symbol}.txt\")\n",
        "    with open(regression_filename, 'w') as f:\n",
        "        f.write(summary_str)"
      ],
      "metadata": {
        "id": "_rUemL7XKJLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the folder containing the pairplots\n",
        "!zip -r /content/regression_NewsOnComments.zip /content/regression_NewsOnComments"
      ],
      "metadata": {
        "id": "pNCYo286KiBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes in Reddit Submissions effect News\n",
        "\n",
        "*   Dependent: News\n",
        "*   Predictor: Reddit Submissions"
      ],
      "metadata": {
        "id": "iqSNE4ATLXX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import statsmodels.api as sm\n",
        "\n",
        "regression_dir = '/content/regression_SubmissionOnNews'\n",
        "os.makedirs(regression_dir, exist_ok=True)\n",
        "\n",
        "for symbol in popular_symbols:\n",
        "    # Filter and sort the data by date for the current symbol\n",
        "    symbol_data = merged_df[merged_df['symbol'] == symbol].copy().sort_values('date')\n",
        "    # Drop rows with missing values for news_sentiment or submissions_sentiment\n",
        "    symbol_data = symbol_data.dropna(subset=['news_sentiment', 'submissions_sentiment'])\n",
        "    if symbol_data.empty:\n",
        "        continue\n",
        "\n",
        "    # Set up regression: reddit_submissions ~ news_sentiment\n",
        "    # Independent variable: news sentiment, with a constant added\n",
        "    X = sm.add_constant(symbol_data['submissions_sentiment'])\n",
        "    # Dependent variable: reddit submissions sentiment\n",
        "    y = symbol_data['news_sentiment']\n",
        "\n",
        "    # Fit the OLS regression model\n",
        "    model = sm.OLS(y, X).fit()\n",
        "\n",
        "    # Get the model summary as text\n",
        "    summary_str = model.summary().as_text()\n",
        "\n",
        "    # Save the summary to a text file\n",
        "    regression_filename = os.path.join(regression_dir, f\"regression_{symbol}.txt\")\n",
        "    with open(regression_filename, 'w') as f:\n",
        "        f.write(summary_str)"
      ],
      "metadata": {
        "id": "YOZXS_3-Lf3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the folder containing the pairplots\n",
        "!zip -r /content/regression_SubmissionOnNews.zip /content/regression_SubmissionOnNews"
      ],
      "metadata": {
        "id": "PKznvFcgMPK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes in Reddit Submissions effect News\n",
        "\n",
        "*   Dependent: News\n",
        "*   Predictor: Reddit Comments"
      ],
      "metadata": {
        "id": "ex7Z0reOMzgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import statsmodels.api as sm\n",
        "\n",
        "regression_dir = '/content/regression_CommentsOnNews'\n",
        "os.makedirs(regression_dir, exist_ok=True)\n",
        "\n",
        "for symbol in popular_symbols:\n",
        "    # Filter and sort the data by date for the current symbol\n",
        "    symbol_data = merged_df[merged_df['symbol'] == symbol].copy().sort_values('date')\n",
        "    # Drop rows with missing values for news_sentiment or submissions_sentiment\n",
        "    symbol_data = symbol_data.dropna(subset=['news_sentiment', 'comments_sentiment'])\n",
        "    if symbol_data.empty:\n",
        "        continue\n",
        "\n",
        "    # Set up regression: reddit_submissions ~ news_sentiment\n",
        "    # Independent variable: news sentiment, with a constant added\n",
        "    X = sm.add_constant(symbol_data['comments_sentiment'])\n",
        "    # Dependent variable: reddit submissions sentiment\n",
        "    y = symbol_data['news_sentiment']\n",
        "\n",
        "    # Fit the OLS regression model\n",
        "    model = sm.OLS(y, X).fit()\n",
        "\n",
        "    # Get the model summary as text\n",
        "    summary_str = model.summary().as_text()\n",
        "\n",
        "    # Save the summary to a text file\n",
        "    regression_filename = os.path.join(regression_dir, f\"regression_{symbol}.txt\")\n",
        "    with open(regression_filename, 'w') as f:\n",
        "        f.write(summary_str)"
      ],
      "metadata": {
        "id": "XwgU1n9XM3A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the folder containing the pairplots\n",
        "!zip -r /content/regression_CommentsOnNews.zip /content/regression_CommentsOnNews"
      ],
      "metadata": {
        "id": "X2K1cCIsM-eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Causality:\n",
        "\n",
        "If the Granger causality test suggests that Reddit transitions “cause” (in a forecasting sense) news transitions (e.g., p-values < 0.05), this may imply that Reddit sentiment changes precede changes in news sentiment. Compare the results for submissions versus comments to see which has a stronger predictive relationship."
      ],
      "metadata": {
        "id": "rZYvYbAEOR-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reddit Granger Causes News\n",
        "\n",
        "Submissions Predictive relationship for News\n",
        "Comments Predictive relationship for News"
      ],
      "metadata": {
        "id": "_3ek0YVaOmMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "granger_dir = '/content/granger_RedditOnNews'\n",
        "\n",
        "os.makedirs(granger_dir, exist_ok=True)\n",
        "\n",
        "maxlag = 4  # Maximum lag for Granger causality tests\n",
        "\n",
        "for symbol in popular_symbols:\n",
        "    symbol_data = merged_df[merged_df['symbol'] == symbol].copy().sort_values('date')\n",
        "    # Use only the necessary columns and drop rows with missing data\n",
        "    test_data = symbol_data[['news_sentiment', 'submissions_sentiment', 'comments_sentiment']].dropna()\n",
        "    if test_data.empty:\n",
        "        continue\n",
        "\n",
        "    # Capture the Granger test outputs\n",
        "    granger_output = io.StringIO()\n",
        "    with redirect_stdout(granger_output):\n",
        "        print(f\"Granger Causality Tests for {symbol}:\\n\")\n",
        "        print(\"Testing if submissions_sentiment Granger-causes news_sentiment:\")\n",
        "        tsastat.grangercausalitytests(test_data[['news_sentiment', 'submissions_sentiment']], maxlag=maxlag, verbose=True)\n",
        "        print(\"\\nTesting if comments_sentiment Granger-causes news_sentiment:\")\n",
        "        tsastat.grangercausalitytests(test_data[['news_sentiment', 'comments_sentiment']], maxlag=maxlag, verbose=True)\n",
        "\n",
        "    granger_text = granger_output.getvalue()\n",
        "    granger_filename = os.path.join(granger_dir, f\"granger_{symbol}.txt\")\n",
        "    with open(granger_filename, 'w') as f:\n",
        "        f.write(granger_text)\n",
        "    granger_output.close()"
      ],
      "metadata": {
        "id": "MwenIYdDORDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the folder containing the pairplots\n",
        "!zip -r /content/granger_RedditOnNews.zip /content/granger_RedditOnNews\n",
        "\n"
      ],
      "metadata": {
        "id": "molBOME_O4cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "News Granger Causes Reddit\n",
        "\n",
        "News Predictive relationship for submissions\n",
        "News Predictive relationship for comments"
      ],
      "metadata": {
        "id": "KUb0e8VAO9Qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "granger_dir = '/content/granger_NewsOnReddit'\n",
        "\n",
        "os.makedirs(granger_dir, exist_ok=True)\n",
        "\n",
        "maxlag = 4  # Maximum lag for Granger causality tests\n",
        "\n",
        "for symbol in popular_symbols:\n",
        "    symbol_data = merged_df[merged_df['symbol'] == symbol].copy().sort_values('date')\n",
        "    # Use only the necessary columns and drop rows with missing data\n",
        "    test_data = symbol_data[['news_sentiment', 'submissions_sentiment', 'comments_sentiment']].dropna()\n",
        "    if test_data.empty:\n",
        "        continue\n",
        "\n",
        "    # Capture the Granger test outputs\n",
        "    granger_output = io.StringIO()\n",
        "    with redirect_stdout(granger_output):\n",
        "        print(f\"Granger Causality Tests for {symbol}:\\n\")\n",
        "        print(\"Testing if news_sentiment Granger-causes submissions_sentiment:\")\n",
        "        tsastat.grangercausalitytests(test_data[['submissions_sentiment', 'news_sentiment']], maxlag=maxlag, verbose=True)\n",
        "\n",
        "        print(\"\\nTesting if news_sentiment Granger-causes comments_sentiment :\")\n",
        "        tsastat.grangercausalitytests(test_data[['comments_sentiment', 'news_sentiment']], maxlag=maxlag, verbose=True)\n",
        "\n",
        "    granger_text = granger_output.getvalue()\n",
        "    granger_filename = os.path.join(granger_dir, f\"granger_{symbol}.txt\")\n",
        "    with open(granger_filename, 'w') as f:\n",
        "        f.write(granger_text)\n",
        "    granger_output.close()"
      ],
      "metadata": {
        "id": "QeTHoG81PAtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the folder containing the pairplots\n",
        "!zip -r /content/granger_NewsOnReddit.zip /content/granger_NewsOnReddit"
      ],
      "metadata": {
        "id": "E2A0JhV4QCzM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}