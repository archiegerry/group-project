{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jgY3LD0lHMa"
      },
      "outputs": [],
      "source": [
        "#numbers and data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "import openpyxl\n",
        "\n",
        "#roberta\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from scipy.special import softmax\n",
        "import torch\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "import math\n",
        "\n",
        "#vader\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0DLVemejlYrj"
      },
      "outputs": [],
      "source": [
        "# 1. Running on Google Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# %ls /content/drive/MyDrive/Group_project/Code/sample_news\n",
        "# %cd /content/drive/MyDrive/Group_project/Code/sample_news\n",
        "# csv_folder = '/content/drive/MyDrive/Group_project/Code/sample_news/'\n",
        "\n",
        "\n",
        "# 2. Running on VScode\n",
        "csv_folder = 'sample_news/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "x5M8ZsZ_i8sg"
      },
      "outputs": [],
      "source": [
        "#Sample news data that was used for our preliminary model testing\n",
        "pattern = os.path.join(csv_folder, '*.csv')\n",
        "file_list = glob.glob(pattern)\n",
        "\n",
        "df_list = []\n",
        "for fp in file_list:\n",
        "    df = pd.read_csv(fp)\n",
        "    source = os.path.basename(fp).replace('.csv','').split('_')[1]\n",
        "    df['source'] = source\n",
        "    df_list.append(df)\n",
        "\n",
        "all_data = pd.concat(df_list, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0KtehILmUnw"
      },
      "outputs": [],
      "source": [
        "print(all_data.columns)\n",
        "title_data = all_data[['Title', 'Published']]\n",
        "body_data = all_data[['Content', 'Published']].copy()\n",
        "body_data = body_data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5v2TBA4IoiD"
      },
      "outputs": [],
      "source": [
        "#VADER\n",
        "sid = SIA()\n",
        "\n",
        "#body\n",
        "body_data['vader sentiments']           = body_data['Content'].astype(str).apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\n",
        "body_data['vader Positive Sentiment']   = body_data['vader sentiments'].apply(lambda x: x['pos']+1*(10**-6))\n",
        "body_data['vader Neutral Sentiment']    = body_data['vader sentiments'].apply(lambda x: x['neu']+1*(10**-6))\n",
        "body_data['vader Negative Sentiment']   = body_data['vader sentiments'].apply(lambda x: x['neg']+1*(10**-6))\n",
        "body_data['vader Compound Sentiment']   = body_data['vader sentiments'].apply(lambda x: x['compound']+1*(10**-6))\n",
        "body_data.drop(columns=['vader sentiments'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "v-f6MJnnZagj"
      },
      "outputs": [],
      "source": [
        "#RoBERTa\n",
        "\n",
        "#connect to GPU\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"Using CPU\")\n",
        "\n",
        "#model name\n",
        "#below are the different RoBERTa models we chose to test\n",
        "#MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "#MODEL = f\"ProsusAI/finbert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(device)\n",
        "\n",
        "#function to split chunks if needed since roberta can do 512 at max (max 512 tokens can be processed at a time)\n",
        "def analyze_large_text(text):\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)  # Encode to token IDs without special tokens\n",
        "    chunk_size = 512\n",
        "    # Overlap between chunks to avoid missing context between chunks\n",
        "    stride = 256\n",
        "    sentiment_scores = []\n",
        "    token_lengths = []\n",
        "\n",
        "    for i in range(0, len(tokens), stride):\n",
        "        chunk = tokens[i:min(i + chunk_size, len(tokens))]\n",
        "        chunk_text = tokenizer.decode(chunk)  # Decode back to text\n",
        "\n",
        "        inputs = tokenizer(chunk_text, return_tensors='pt', truncation=True, max_length=512).to(device)\n",
        "        output = model(**inputs)\n",
        "        scores = output[0][0].detach().cpu().numpy()\n",
        "        scores = softmax(scores)\n",
        "        sentiment_scores.append(scores)\n",
        "        token_lengths.append(len(chunk))\n",
        "\n",
        "    # Weighted average of scores by chunk length\n",
        "    sentiment_scores = np.array(sentiment_scores)\n",
        "    weighted_scores = np.average(sentiment_scores, axis=0, weights=token_lengths)\n",
        "\n",
        "    compound_score = (weighted_scores[2] - weighted_scores[0])\n",
        "\n",
        "    #decrease the score to 20 ish\n",
        "    normalised_compound = compound_score / math.sqrt((compound_score*compound_score)+3)\n",
        "\n",
        "    # Return final aggregated sentiment\n",
        "    # we want all of this data\n",
        "    return {\n",
        "        'roberta_pos': weighted_scores[2], #these are not softmaxed\n",
        "        'roberta_neu': weighted_scores[1],\n",
        "        'roberta_neg': weighted_scores[0],\n",
        "        'roberta_compound': compound_score, #difference between pos and neg\n",
        "        'roberta_normalised_compound': normalised_compound\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "df[['roberta_pos', 'roberta_neu', 'roberta_neg', 'roberta_compound', 'roberta_normalised_compound']] = df['full data'].apply(lambda x: pd.Series(analyze_large_text(x)))\n",
        "body_data[['roberta_pos', 'roberta_neu', 'roberta_neg', 'roberta_compound', 'roberta_normalised_compound']] = body_data['Content'].apply(lambda x: pd.Series(analyze_large_text(x)))\n",
        "\n",
        "\n",
        "# print(news_data.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LZcTmGECRRz"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "g1WjFst87VAQ"
      },
      "outputs": [],
      "source": [
        "#Roberta ENSEMBLE MODEL - combines 'Sentiment-Latest' with 'ProsusAI/finbert'\n",
        "\n",
        "# Device setup (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load both sentiment models\n",
        "MODEL_1 = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"  # Social media sentiment model\n",
        "MODEL_2 = \"ProsusAI/finbert\"  # Financial sentiment model\n",
        "\n",
        "tokenizer_1 = AutoTokenizer.from_pretrained(MODEL_1)\n",
        "model_1 = AutoModelForSequenceClassification.from_pretrained(MODEL_1).to(device)\n",
        "\n",
        "tokenizer_2 = AutoTokenizer.from_pretrained(MODEL_2)\n",
        "model_2 = AutoModelForSequenceClassification.from_pretrained(MODEL_2).to(device)\n",
        "\n",
        "# Function to get sentiment scores for a given model\n",
        "def get_sentiment_scores(model, tokenizer, text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512).to(device)\n",
        "    output = model(**inputs)\n",
        "    scores = output.logits.detach().cpu().numpy()[0]\n",
        "    return softmax(scores)  # Normalize the scores\n",
        "\n",
        "# Function to analyse large text chunks\n",
        "def analyse_large_text_e(text):\n",
        "    tokens = tokenizer_1.encode(text, add_special_tokens=False)\n",
        "    chunk_size = 512\n",
        "    stride = 256\n",
        "    sentiment_scores_1 = []\n",
        "    sentiment_scores_2 = []\n",
        "    token_lengths = []\n",
        "\n",
        "    for i in range(0, len(tokens), stride):\n",
        "        chunk = tokens[i:min(i + chunk_size, len(tokens))]\n",
        "        chunk_text = tokenizer_1.decode(chunk)\n",
        "\n",
        "        # Get sentiment scores from both models\n",
        "        scores_1 = get_sentiment_scores(model_1, tokenizer_1, chunk_text)\n",
        "        scores_2 = get_sentiment_scores(model_2, tokenizer_2, chunk_text)\n",
        "\n",
        "        sentiment_scores_1.append(scores_1)\n",
        "        sentiment_scores_2.append(scores_2)\n",
        "        token_lengths.append(len(chunk))\n",
        "\n",
        "    # Weighted average of scores (based on token lengths)\n",
        "    sentiment_scores_1 = np.array(sentiment_scores_1)\n",
        "    sentiment_scores_2 = np.array(sentiment_scores_2)\n",
        "\n",
        "    weighted_scores_1 = np.average(sentiment_scores_1, axis=0, weights=token_lengths)\n",
        "    weighted_scores_2 = np.average(sentiment_scores_2, axis=0, weights=token_lengths)\n",
        "\n",
        "    compund_score_1 = (weighted_scores_1[2] - weighted_scores_1[0])\n",
        "    compund_score_2 = (weighted_scores_2[2] - weighted_scores_2[0])\n",
        "\n",
        "    normalised_compound_1 = compund_score_1 / math.sqrt((compund_score_1*compund_score_1)+20)\n",
        "    normalised_compound_2 = compund_score_2 / math.sqrt((compund_score_2*compund_score_2)+20)\n",
        "\n",
        "    avg_compound = (normalised_compound_1 * 3 + normalised_compound_2 * 2) / 5\n",
        "\n",
        "    compound_score_ensemble = (weighted_scores_1[2] + weighted_scores_2[2] - weighted_scores_1[0] - weighted_scores_2[0])\n",
        "\n",
        "    normalised_compound = compound_score_ensemble / math.sqrt((compound_score_ensemble*compound_score_ensemble)+20)\n",
        "\n",
        "\n",
        "\n",
        "    # Return final aggregated sentiment\n",
        "    # Final ensemble output (Average scores from both models)\n",
        "    final_scores = (weighted_scores_1 + weighted_scores_2) / 2\n",
        "\n",
        "    # Return final sentiment scores\n",
        "    return {\n",
        "        'ensemble_pos': final_scores[2],  # Positive\n",
        "        'ensemble_neu': final_scores[1],  # Neutral\n",
        "        'ensemble_neg': final_scores[0],  # Negative\n",
        "        'normalised_compound_1': normalised_compound_1,\n",
        "        'normalised_compound_2': normalised_compound_2,\n",
        "        'avg_compound': avg_compound,\n",
        "        'normalised_compound': normalised_compound,\n",
        "    }\n",
        "\n",
        "# # Apply sentiment analysis to title and body data\n",
        "body_data[['ensemble_pos', 'ensemble_neu', 'ensemble_neg', 'normalised_compound_1', 'normalised_compound_2','avg_compound', 'normalised_compound']] = body_data['Content'].apply(lambda x: pd.Series(analyse_large_text_e(x)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAAQ9VL8ph9p"
      },
      "outputs": [],
      "source": [
        "# Convert 'Published' to datetime format and set as index for easier plotting\n",
        "\n",
        "body_data['Sentiment_Diff_Ensemble'] = body_data['ensemble_pos'] - body_data['ensemble_neg']\n",
        "# body_data['Sentiment_Diff_Vader'] = body_data['vader Positive Sentiment'] - body_data['vader Negative Sentiment']\n",
        "# body_data['Sentiment_Diff_BERT'] = body_data['roberta_pos'] - body_data['roberta_neg']\n",
        "\n",
        "\n",
        "body_data['Published'] = pd.to_datetime(body_data['Published'])\n",
        "body_data.set_index('Published', drop=False, inplace=True)\n",
        "\n",
        "# Plot sentiment scores over time for both models\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Scatter plots\n",
        "# plt.scatter(body_data.index, body_data['Sentiment_Diff_BERT'], label='RoBERTa', color='red')\n",
        "# plt.scatter(body_data.index, body_data['Sentiment_Diff_Vader'], label='VADER', color='blue')\n",
        "plt.scatter(body_data.index, body_data['Sentiment_Diff_Ensemble'], label='Ensemble', color='blue')\n",
        "\n",
        "# # Linear regression f\n",
        "x = body_data.index.map(lambda x: x.toordinal())  # Convert datetime to ordinal for fitting\n",
        "y = body_data['Sentiment_Diff_Ensemble']\n",
        "m, b = np.polyfit(x, y, 1)\n",
        "plt.plot(body_data.index, m * x + b, color='blue', linestyle='--', label='BERT Trend') #change labels accordingly\n",
        "\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sentiment Score')\n",
        "plt.title('Sentiment Analysis of GameStop News Articles Over Time')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2_Wxv0b_Hrw"
      },
      "outputs": [],
      "source": [
        "#Creating histograms to visualise the distribution of scores across different models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the distribution of Sentiment_Diff_Vader\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(body_data['Sentiment_Diff_Ensemble'], bins=20, edgecolor='black', color='skyblue')\n",
        "\n",
        "# Styling the plot\n",
        "plt.title('Distribution of the Ensemble Sentiment Difference Scores')\n",
        "plt.xlabel('Sentiment Difference (Positive - Negative)')\n",
        "plt.ylabel('Number of Articles')\n",
        "plt.grid(True)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEApyi1i5-_W"
      },
      "source": [
        "The code below shows how we calculate MAE and RMSE scores of each model in comparison to the ground-truths extracted from our user-testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuaK_j6J53EY"
      },
      "outputs": [],
      "source": [
        "#MAE and RMSE of model scores\n",
        "#https://saturncloud.io/blog/how-to-read-data-from-google-sheets-using-colaboratory-google/\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Option 1: Google drive:\n",
        "# url = 'https://docs.google.com/spreadsheets/d/1iTP8NtWOMIInWUcqQKfIJcbzzVTLLkJhrlzgjkB86S4/export?format=csv'\n",
        "# df = pd.read_csv(url)\n",
        "\n",
        "# Option 2: VScode:\n",
        "file = 'Model_comparison.xlsx'\n",
        "df = pd.read_excel(file)\n",
        "\n",
        "df = df.dropna()\n",
        "\n",
        "\n",
        "df.head()\n",
        "\n",
        "# Select the last 5 columns\n",
        "model_and_truth = df.iloc[:, -5:]\n",
        "\n",
        "# Split models and ground-truth\n",
        "model_outputs = model_and_truth.iloc[:, :-1]  # First 4 columns = model outputs\n",
        "user_avg_scores = model_and_truth.iloc[:, -1] # Last column = average user rating\n",
        "\n",
        "#Scale data so it makes sense\n",
        "# Step 1: Rescale model outputs from [-1,1] to [1,5]\n",
        "model_outputs_scaled = (model_outputs + 1) * 2\n",
        "model_outputs_scaled = model_outputs_scaled + 1\n",
        "\n",
        "# Calculate MAE and RMSE\n",
        "mae_scores = (model_outputs_scaled.sub(user_avg_scores, axis=0)).abs().mean()\n",
        "rmse_scores = np.sqrt((model_outputs_scaled.sub(user_avg_scores, axis=0) ** 2).mean())\n",
        "\n",
        "# Combine and display results\n",
        "error_metrics = pd.DataFrame({\n",
        "    'MAE': mae_scores,\n",
        "    'RMSE': rmse_scores\n",
        "})\n",
        "\n",
        "print(error_metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
