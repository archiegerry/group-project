{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29441,
     "status": "ok",
     "timestamp": 1739375243079,
     "user": {
      "displayName": "Aaditi Agrawal",
      "userId": "02382142628344544705"
     },
     "user_tz": 0
    },
    "id": "dgz77iaHsNbz",
    "outputId": "e3d6636e-d51a-4b08-e175-359005c0476b"
   },
   "outputs": [],
   "source": [
    "# !python -m pip install boto3 python-dotenv transformers torch joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739375243085,
     "user": {
      "displayName": "Aaditi Agrawal",
      "userId": "02382142628344544705"
     },
     "user_tz": 0
    },
    "id": "TjLCBnjnsdDk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "# Roberta model\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "import math\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup S3 on VSCode (recommended) or GoogleDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Running on VSCode:\n",
    "scripts_folder = os.path.join(os.getcwd(), '..', 'Scripts')\n",
    "sys.path.append(scripts_folder)\n",
    "import s3\n",
    "\n",
    "env_path = \"../Scripts/.env\"\n",
    "load_dotenv(env_path)\n",
    "\n",
    "\n",
    "# 2. Running on Google Drive:\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# env_path = \"<path_here>\"\n",
    "# load_dotenv(env_path)\n",
    "# %cd /content/drive/MyDrive/Group_project/Code/\n",
    "# import s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download Required Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1739375244167,
     "user": {
      "displayName": "Aaditi Agrawal",
      "userId": "02382142628344544705"
     },
     "user_tz": 0
    },
    "id": "HgezHLi0s5MR"
   },
   "outputs": [],
   "source": [
    "data_to_download = \"processed/news/gnews_artifacts/\" # could instead use \"processed/reddit/comments_artifacts/\" or other dataset in same format\n",
    "file_path = \"processed/news/gnews_artifacts/\"\n",
    "\n",
    "s3.download_all(data_to_download)\n",
    "\n",
    "dfs = [\n",
    "    (f, pd.read_parquet(f)) for f in s3.s3_to_local_path(data_to_download).glob(\"*\")\n",
    "]\n",
    "print(f\"{sum(len(df) for _, df in dfs)} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setup Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 23073,
     "status": "ok",
     "timestamp": 1739375398097,
     "user": {
      "displayName": "Aaditi Agrawal",
      "userId": "02382142628344544705"
     },
     "user_tz": 0
    },
    "id": "zEd-JScquDmR",
    "outputId": "27196ff7-fc37-41ca-9d73-cc70e5617f24"
   },
   "outputs": [],
   "source": [
    "# Connect to GPU\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "  print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "  print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define Model and Related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(device)\n",
    "\n",
    "# Function to get sentiment scores for a chunk\n",
    "def get_sentiment_scores(chunk):\n",
    "    encoded_text = tokenizer(chunk, return_tensors='pt')\n",
    "    output = model(**encoded_text)\n",
    "    #extracting and normalising sentiment scores\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    #normalising the score\n",
    "    return softmax(scores)\n",
    "\n",
    "# Function to split chunks if needed since roberta can do 512 at max (max 512 tokens can be processed at a time)\n",
    "def analyse_large_text(text):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)  # Encode to token IDs without special tokens\n",
    "    chunk_size = 512\n",
    "    # Overlap between chunks to avoid missing context between chunks\n",
    "    stride = 256\n",
    "    sentiment_scores = []\n",
    "    token_lengths = []\n",
    "\n",
    "    for i in range(0, len(tokens), stride):\n",
    "        chunk = tokens[i:min(i + chunk_size, len(tokens))]\n",
    "        chunk_text = tokenizer.decode(chunk)  # Decode back to text\n",
    "        inputs = tokenizer(chunk_text, return_tensors='pt', truncation=True, max_length=512).to(device)\n",
    "        output = model(**inputs)\n",
    "        scores = output[0][0].detach().cpu().numpy()\n",
    "        sentiment_scores.append(scores)\n",
    "        token_lengths.append(len(chunk))\n",
    "\n",
    "    # Weighted average of scores by chunk length\n",
    "    sentiment_scores = np.array(sentiment_scores)\n",
    "    weighted_scores = np.average(sentiment_scores, axis=0, weights=token_lengths)\n",
    "    compound_score = weighted_scores[2] - weighted_scores[0]\n",
    "    normalised_compound = compound_score / math.sqrt(compound_score**2 + 20)\n",
    "\n",
    "    # Return final aggregated sentiment\n",
    "    return {\n",
    "        'roberta_pos': weighted_scores[2],\n",
    "        'roberta_neu': weighted_scores[1],\n",
    "        'roberta_neg': weighted_scores[0],\n",
    "        'roberta_compound': compound_score,\n",
    "        'roberta_normalised_compound': normalised_compound,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Apply Model on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paths = []\n",
    "for path, df in tqdm(dfs):\n",
    "    out_path = Path(str(path).replace(\"artifacts\", \"twitter_roberta\")) # Assumes that there is 'artifacts' in input path\n",
    "    out_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    all_paths.append(out_path)\n",
    "    if not out_path.exists():\n",
    "        print(out_path)\n",
    "        df[['roberta_pos', 'roberta_neu', 'roberta_neg', 'roberta_compound', 'roberta_normalised_compound']] = df['text'].apply(lambda x: pd.Series(analyse_large_text(x)))\n",
    "        df.to_parquet(out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_all(file_path.replace(\"artifacts\", \"twitter_roberta\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMAR68WB+/E1jzrhC9K2QcP",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
